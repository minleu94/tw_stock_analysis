{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d31f8ea-ec11-45fc-85b4-4b9333293f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c365497-6252-42d5-bb12-5739d2995aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Union, Optional, Tuple\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27cb43b-e6b3-4efd-ad67-b594b63a3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPaths:\n",
    "    \"\"\"模型相關路徑管理\"\"\"\n",
    "    BASE_DIR = Path(\"D:/Min/Python/Project/FA_Data\")\n",
    "    MODEL_DIR = BASE_DIR / \"models\"\n",
    "    META_DIR = BASE_DIR / \"meta_data\"\n",
    "    FEATURE_DIR = BASE_DIR / \"features\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\"\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls) -> None:\n",
    "        \"\"\"創建必要目錄結構\"\"\"\n",
    "        directories = [cls.MODEL_DIR, cls.META_DIR, cls.FEATURE_DIR, cls.RESULTS_DIR]\n",
    "        for directory in directories:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_path(cls, stock_id: str, model_type: str) -> Path:\n",
    "        \"\"\"獲取模型儲存路徑\"\"\"\n",
    "        return cls.MODEL_DIR / f\"{stock_id}_{model_type}_model.pkl\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_result_path(cls, stock_id: str, model_type: str) -> Path:\n",
    "        \"\"\"獲取結果儲存路徑\"\"\"\n",
    "        return cls.RESULTS_DIR / f\"{stock_id}_{model_type}_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c8d810b-1d89-4e95-be95-4be30a0855f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    \"\"\"模型基礎類\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"初始化\"\"\"\n",
    "        self.model = None\n",
    "        self.train_time = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        訓練模型\n",
    "        \n",
    "        Args:\n",
    "            X: 特徵矩陣\n",
    "            y: 標籤向量\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"開始訓練模型，數據形狀: X={X.shape}, y={y.shape}\")\n",
    "            start_time = datetime.now()\n",
    "            self.model.fit(X, y)\n",
    "            self.train_time = (datetime.now() - start_time).total_seconds()\n",
    "            self.logger.info(f\"模型訓練完成，耗時: {self.train_time:.2f}秒\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"訓練過程出錯: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        預測標籤\n",
    "        \n",
    "        Args:\n",
    "            X: 特徵矩陣\n",
    "            \n",
    "        Returns:\n",
    "            預測的標籤值\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"模型尚未訓練\")\n",
    "            \n",
    "            self.logger.info(f\"開始預測，輸入數據形狀: {X.shape}\")\n",
    "            predictions = self.model.predict(X)\n",
    "            self.logger.info(f\"預測完成，輸出形狀: {predictions.shape}\")\n",
    "            return predictions\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"預測過程出錯: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        預測概率\n",
    "        \n",
    "        Args:\n",
    "            X: 特徵矩陣\n",
    "            \n",
    "        Returns:\n",
    "            預測的概率值\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"模型尚未訓練\")\n",
    "                \n",
    "            self.logger.info(f\"開始預測概率，輸入數據形狀: {X.shape}\")\n",
    "            probabilities = self.model.predict_proba(X)\n",
    "            self.logger.info(f\"預測完成，輸出形狀: {probabilities.shape}\")\n",
    "            return probabilities\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"預測概率過程出錯: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def save(self, path: Path) -> None:\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        try:\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 區分一般模型和集成模型\n",
    "            if isinstance(self, EnsembleModel):\n",
    "                save_data = {\n",
    "                    'model_type': 'ensemble',\n",
    "                    'sub_models': [\n",
    "                        {\n",
    "                            'model': model.model,\n",
    "                            'train_time': model.train_time\n",
    "                        }\n",
    "                        for model in self.models\n",
    "                    ],\n",
    "                    'weights': self.weights,\n",
    "                    'train_time': self.train_time\n",
    "                }\n",
    "            elif self.model is None:\n",
    "                raise ValueError(\"沒有可保存的模型\")\n",
    "            else:\n",
    "                save_data = {\n",
    "                    'model_type': 'single',\n",
    "                    'model': self.model,\n",
    "                    'train_time': self.train_time\n",
    "                }\n",
    "                \n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_data, f)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"保存模型時出錯: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> 'BaseModel':\n",
    "        \"\"\"載入模型\"\"\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"找不到模型文件: {path}\")\n",
    "            \n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if data.get('model_type') == 'ensemble':\n",
    "            # 重建子模型\n",
    "            sub_models = []\n",
    "            for sub_model_data in data['sub_models']:\n",
    "                model = cls()\n",
    "                model.model = sub_model_data['model']\n",
    "                model.train_time = sub_model_data['train_time']\n",
    "                sub_models.append(model)\n",
    "                \n",
    "            instance = EnsembleModel(\n",
    "                models=sub_models,\n",
    "                weights=data['weights']\n",
    "            )\n",
    "        else:\n",
    "            instance = cls()\n",
    "            instance.model = data['model']\n",
    "            instance.train_time = data['train_time']\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "    def get_feature_importance(self, feature_names: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        獲取特徵重要性（如果模型支持）\n",
    "        \n",
    "        Args:\n",
    "            feature_names: 特徵名稱列表\n",
    "            \n",
    "        Returns:\n",
    "            特徵重要性字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"模型尚未訓練\")\n",
    "                \n",
    "            if not hasattr(self.model, 'feature_importances_'):\n",
    "                self.logger.warning(\"當前模型不支持特徵重要性\")\n",
    "                return {}\n",
    "                \n",
    "            self.logger.info(\"開始計算特徵重要性\")\n",
    "            importance_dict = dict(zip(feature_names, self.model.feature_importances_))\n",
    "            self.logger.info(\"特徵重要性計算完成\")\n",
    "            return importance_dict\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取特徵重要性時出錯: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94893cd9-5267-4ef5-86d9-c85f173f0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory:\n",
    "    \"\"\"模型工廠類\"\"\"\n",
    "    MODEL_MAPPINGS = {\n",
    "        'random_forest': (RandomForestClassifier, {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }),\n",
    "        'xgboost': (xgb.XGBClassifier, {\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'random_state': 42\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def create_model(cls, model_type: str, params: Optional[Dict] = None) -> BaseModel:\n",
    "        \"\"\"創建模型實例\"\"\"\n",
    "        model_type = model_type.lower()\n",
    "        if model_type not in cls.MODEL_MAPPINGS:\n",
    "            raise ValueError(f\"不支援的模型類型: {model_type}\")\n",
    "            \n",
    "        model_class, default_params = cls.MODEL_MAPPINGS[model_type]\n",
    "        final_params = {**default_params, **(params or {})}\n",
    "        \n",
    "        # 創建一個基礎模型實例\n",
    "        model = BaseModel()\n",
    "        model.model = model_class(**final_params)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68028165-08e5-44b4-9b22-bd37f27b90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"模型評估器\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_classification(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                              y_prob: Optional[np.ndarray] = None) -> Dict[str, float]:\n",
    "        \"\"\"評估分類模型效果\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        if y_prob is not None:\n",
    "            metrics['auc'] = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_feature_stability(X_train: np.ndarray, X_test: np.ndarray, \n",
    "                                 feature_names: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"評估特徵穩定性\"\"\"\n",
    "        eps = 1e-10\n",
    "        stability_scores = {}\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            train_dist = np.histogram(X_train[:, i], bins=20)[0] + eps\n",
    "            test_dist = np.histogram(X_test[:, i], bins=20)[0] + eps\n",
    "            psi = np.sum((train_dist - test_dist) * np.log(train_dist / test_dist))\n",
    "            stability_scores[feature] = psi\n",
    "        return stability_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_prediction_distribution(y_prob: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"評估預測分布\"\"\"\n",
    "        return {\n",
    "            'prediction_mean': float(np.mean(y_prob[:, 1])),\n",
    "            'prediction_std': float(np.std(y_prob[:, 1])),\n",
    "            'prediction_skew': float(stats.skew(y_prob[:, 1])),\n",
    "            'prediction_kurt': float(stats.kurtosis(y_prob[:, 1]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0967b9a-9b69-4ecb-94d2-8908aec19c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStabilityChecker:\n",
    "    \"\"\"檢查模型穩定性\"\"\"\n",
    "    \n",
    "    def check_model_stability(self, model: BaseModel, X: np.ndarray, y: np.ndarray, \n",
    "                            n_splits: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"檢查模型穩定性\"\"\"\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        prediction_variances = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            model_fold = copy.deepcopy(model)\n",
    "            model_fold.train(X_train, y_train)\n",
    "            y_pred_proba = model_fold.predict_proba(X_val)\n",
    "            y_pred = (y_pred_proba[:, 1] >= 0.5).astype(int)\n",
    "            \n",
    "            fold_scores.append(accuracy_score(y_val, y_pred))\n",
    "            prediction_variances.append(np.var(y_pred_proba[:, 1]))\n",
    "        \n",
    "        return {\n",
    "            'score_mean': float(np.mean(fold_scores)),\n",
    "            'score_std': float(np.std(fold_scores)),\n",
    "            'score_cv': float(np.std(fold_scores) / np.mean(fold_scores)),\n",
    "            'prediction_variance': float(np.mean(prediction_variances))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae9e63f1-d165-40e9-ad06-7425d57bdb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptimizer:\n",
    "    \"\"\"模型優化器\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str, param_grid: Dict[str, List[Any]]):\n",
    "        self.model_type = model_type\n",
    "        self.param_grid = param_grid\n",
    "        self.best_params = None\n",
    "        self.best_score = None\n",
    "        \n",
    "    def optimize(self, X: np.ndarray, y: np.ndarray, \n",
    "                cv: int = 5, scoring: str = 'accuracy') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        執行參數優化\n",
    "        \n",
    "        Args:\n",
    "            X: 特徵矩陣\n",
    "            y: 標籤向量\n",
    "            cv: 交叉驗證折數\n",
    "            scoring: 評分標準\n",
    "            \n",
    "        Returns:\n",
    "            優化結果字典\n",
    "        \"\"\"\n",
    "        logging.info(f\"開始對 {self.model_type} 進行參數優化...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            ModelFactory.create_model(self.model_type).model,\n",
    "            self.param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.best_score = grid_search.best_score_\n",
    "        \n",
    "        logging.info(f\"優化完成. 最佳分數: {self.best_score:.4f}\")\n",
    "        logging.info(f\"最佳參數: {self.best_params}\")\n",
    "        \n",
    "        return {\n",
    "            'best_params': self.best_params,\n",
    "            'best_score': self.best_score,\n",
    "            'cv_results': grid_search.cv_results_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e26d1bd-891e-40d1-8443-ad6ab8d1a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"數據驗證類\"\"\"\n",
    "    \n",
    "    def __init__(self, stock_id: str, base_path: Path):\n",
    "        self.stock_id = str(stock_id).zfill(4)  # 確保股票代碼格式正確\n",
    "        self.base_path = base_path\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "    def validate_and_load(self) -> pd.DataFrame:\n",
    "        \"\"\"驗證並載入數據\"\"\"\n",
    "        try:\n",
    "            # 檢查前置條件\n",
    "            self._check_prerequisites()\n",
    "            \n",
    "            # 載入數據\n",
    "            stock_data = self._load_data()\n",
    "            \n",
    "            # 驗證數據\n",
    "            self._validate_data(stock_data)\n",
    "            \n",
    "            return stock_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"數據驗證和載入失敗: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _check_prerequisites(self) -> None:\n",
    "        \"\"\"檢查必要的前置數據文件是否存在\"\"\"\n",
    "        # 檢查原始交易數據\n",
    "        stock_data_file = self.base_path / \"meta_data\" / \"stock_data_whole.csv\"\n",
    "        if not stock_data_file.exists():\n",
    "            raise FileNotFoundError(\"找不到股票交易數據，請先執行 01_stock_data_collector.ipynb\")\n",
    "            \n",
    "        # 檢查技術指標數據\n",
    "        tech_indicator_file = self.base_path / \"technical_analysis\" / f\"{self.stock_id}_indicators.csv\"\n",
    "        if not tech_indicator_file.exists():\n",
    "            raise FileNotFoundError(\"找不到技術指標數據，請執行 02_technical_calculator.ipynb\")\n",
    "            \n",
    "        # 檢查特徵數據\n",
    "        feature_file = self.base_path / \"meta_data\" / \"enhanced_features.csv\"\n",
    "        if not feature_file.exists():\n",
    "            raise FileNotFoundError(\"找不到特徵數據，請執行 03_feature_generator.ipynb\")\n",
    "    \n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"載入數據\"\"\"\n",
    "        feature_file = self.base_path / \"meta_data\" / \"enhanced_features.csv\"\n",
    "        \n",
    "        # 檢查文件大小\n",
    "        self.logger.info(f\"特徵文件大小: {feature_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # 讀取數據\n",
    "        stock_data = pd.read_csv(feature_file, dtype={\n",
    "            '證券代號': str,\n",
    "            '日期': str\n",
    "        }, low_memory=False)\n",
    "        \n",
    "        # 基本資訊記錄\n",
    "        self.logger.info(f\"總數據量: {len(stock_data)}\")\n",
    "        self.logger.info(f\"唯一股票數: {stock_data['證券代號'].nunique()}\")\n",
    "        self.logger.info(f\"日期範圍: {stock_data['日期'].min()} 到 {stock_data['日期'].max()}\")\n",
    "        \n",
    "        # 篩選指定股票的數據\n",
    "        stock_data = stock_data[stock_data['證券代號'] == self.stock_id].copy()\n",
    "        self.logger.info(f\"股票 {self.stock_id} 的數據量: {len(stock_data)}\")\n",
    "        \n",
    "        return stock_data\n",
    "    \n",
    "    def _validate_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"驗證數據\"\"\"\n",
    "        # 檢查數據量是否足夠\n",
    "        if len(df) < 30:\n",
    "            raise ValueError(f\"股票 {self.stock_id} 的數據量不足，需要至少30天的數據\")\n",
    "            \n",
    "        # 檢查必要特徵是否存在\n",
    "        required_features = [\n",
    "            '趨勢強度', '振幅', '量能趨勢', '量比', 'RSI', 'MACD',  # 基礎特徵\n",
    "            '均線糾結度', '波動率', 'RSI_動能', 'MACD_動能',        # 技術特徵\n",
    "            'KD_差值', '本益比_相對值', '技術綜合評分'             # 綜合指標\n",
    "        ]\n",
    "        \n",
    "        missing_features = [f for f in required_features if f not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"缺少以下特徵: {missing_features}\")\n",
    "            \n",
    "        # 檢查並報告缺失值\n",
    "        null_stats = df[required_features].isnull().sum()\n",
    "        if null_stats.any():\n",
    "            self.logger.warning(\"數據存在缺失值:\")\n",
    "            for feature, null_count in null_stats[null_stats > 0].items():\n",
    "                self.logger.warning(f\"{feature}: {null_count} 個缺失值\")\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"特徵工程類\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "    def process(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"處理特徵工程\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"開始特徵工程處理\")\n",
    "            \n",
    "            # 清理列名（移除_x, _y後綴）\n",
    "            df = self._clean_column_names(df)\n",
    "            \n",
    "            # 定義必要特徵\n",
    "            required_features = [\n",
    "                '趨勢強度', '振幅', '量能趨勢', '量比', 'RSI', 'MACD',  # 基礎特徵\n",
    "                '均線糾結度', '波動率', 'RSI_動能', 'MACD_動能',        # 技術特徵\n",
    "                'KD_差值', '本益比_相對值', '技術綜合評分'             # 綜合指標\n",
    "            ]\n",
    "            \n",
    "            # 處理缺失值\n",
    "            df = self._handle_missing_values(df, required_features)\n",
    "            \n",
    "            # 準備特徵和標籤\n",
    "            X = df[required_features].values\n",
    "            y = (df['收盤價'].shift(-1) > df['收盤價']).astype(int).values[:-1]\n",
    "            X = X[:-1]  # 移除最後一行，以匹配標籤長度\n",
    "            \n",
    "            self.logger.info(f\"特徵處理完成，特徵形狀: {X.shape}, 標籤形狀: {y.shape}\")\n",
    "            return X, y, required_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"特徵工程處理失敗: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _clean_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"清理列名\"\"\"\n",
    "        df = df.copy()\n",
    "        columns_to_clean = [col for col in df.columns if col.endswith('_x') or col.endswith('_y')]\n",
    "        for col in columns_to_clean:\n",
    "            base_col = col[:-2]\n",
    "            if f\"{base_col}_x\" in df.columns:\n",
    "                df[base_col] = df[f\"{base_col}_x\"]\n",
    "                df.drop([f\"{base_col}_x\", f\"{base_col}_y\"], axis=1, inplace=True, errors='ignore')\n",
    "        return df\n",
    "    \n",
    "    def _handle_missing_values(self, df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"處理缺失值\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature not in df.columns:\n",
    "                self.logger.warning(f\"缺少特徵 {feature}，將被設為0\")\n",
    "                df[feature] = 0\n",
    "            else:\n",
    "                # 使用前向填充和後向填充的組合來處理缺失值\n",
    "                df[feature] = df[feature].ffill().bfill()\n",
    "                \n",
    "                # 如果仍有缺失值，使用均值填充\n",
    "                if df[feature].isnull().any():\n",
    "                    feature_mean = df[feature].mean()\n",
    "                    df[feature] = df[feature].fillna(feature_mean)\n",
    "                    self.logger.info(f\"特徵 {feature} 的剩餘缺失值使用均值 {feature_mean:.4f} 填充\")\n",
    "                    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e2227f7-427a-4bf8-875e-039f665342ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(BaseModel):\n",
    "    \"\"\"混合模型\"\"\"\n",
    "    def __init__(self, models: List[BaseModel], weights: Optional[List[float]] = None):\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "        self.weights = weights or [1/len(models)] * len(models)\n",
    "        self.model = self  # 添加這行,確保 self.model 不是 None\n",
    "        \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"預測概率\"\"\"\n",
    "        predictions = np.zeros((X.shape[0], 2))\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            predictions += model.predict_proba(X) * weight\n",
    "        return predictions\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"訓練所有模型\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        for model in self.models:\n",
    "            model.train(X, y)\n",
    "        self.train_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "    def get_feature_importance(self, feature_names: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"獲取集成模型的特徵重要性\"\"\"\n",
    "        importances = {}\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            model_importance = model.get_feature_importance(feature_names)\n",
    "            for feature, importance in model_importance.items():\n",
    "                importances[feature] = importances.get(feature, 0) + importance * weight\n",
    "        return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6207267-7f7d-4e2f-a88c-6741c596adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"模型訓練管理類\"\"\"\n",
    "    \n",
    "    def __init__(self, stock_id: str, base_path: Optional[Path] = None):\n",
    "        self.stock_id = stock_id\n",
    "        self.base_path = base_path or Path(\"D:/Min/Python/Project/FA_Data\")\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # 參數網格配置\n",
    "        self.param_grids = {\n",
    "            \"random_forest\": {\n",
    "                'n_estimators': [300, 400, 500],\n",
    "                'max_depth': [3, 4, 5, 6],\n",
    "                'min_samples_split': [8, 10, 12],\n",
    "                'min_samples_leaf': [2, 4],\n",
    "                'max_features': ['sqrt', None],\n",
    "                'class_weight': [None, 'balanced']\n",
    "            },\n",
    "            \"xgboost\": {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5, 6],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_child_weight': [1, 3, 5]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def run_batch_training(self, stock_ids: List[str], \n",
    "                         model_types: List[str] = [\"random_forest\", \"xgboost\"],\n",
    "                         optimize: bool = False, \n",
    "                         cv_folds: int = 5,\n",
    "                         batch_size: int = 1000) -> Dict[str, Any]:\n",
    "        \"\"\"執行批次股票訓練\"\"\"\n",
    "        try:\n",
    "            batch_results = {}\n",
    "            \n",
    "            # 1. 先收集所有股票的數據\n",
    "            all_stock_data = {}\n",
    "            for stock_id in stock_ids:\n",
    "                validator = DataValidator(stock_id, self.base_path)\n",
    "                stock_data = validator.validate_and_load()\n",
    "                engineer = FeatureEngineer()\n",
    "                X, y, feature_names = engineer.process(stock_data)\n",
    "                all_stock_data[stock_id] = {\n",
    "                    'X': X,\n",
    "                    'y': y,\n",
    "                    'feature_names': feature_names\n",
    "                }\n",
    "                \n",
    "            # 2. 使用相似股票共同優化參數\n",
    "            if optimize:\n",
    "                # 使用所有股票的數據進行參數優化\n",
    "                combined_X = np.vstack([data['X'] for data in all_stock_data.values()])\n",
    "                combined_y = np.concatenate([data['y'] for data in all_stock_data.values()])\n",
    "                \n",
    "                optimized_params = {}\n",
    "                for model_type in model_types:\n",
    "                    optimizer = ModelOptimizer(model_type, self.param_grids[model_type])\n",
    "                    opt_results = optimizer.optimize(combined_X, combined_y, cv=cv_folds)\n",
    "                    optimized_params[model_type] = opt_results['best_params']\n",
    "            \n",
    "            # 3. 使用優化後的參數進行批次訓練\n",
    "            for stock_id, data in all_stock_data.items():\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    data['X'], data['y'], test_size=0.2, shuffle=False\n",
    "                )\n",
    "                \n",
    "                models = []\n",
    "                for model_type in model_types:\n",
    "                    params = optimized_params[model_type] if optimize else None\n",
    "                    model = ModelFactory.create_model(model_type, params)\n",
    "                    model.train(X_train, y_train)\n",
    "                    models.append(model)\n",
    "                \n",
    "                # 創建並評估集成模型\n",
    "                ensemble_model = EnsembleModel(models)\n",
    "                results = self._evaluate_models(\n",
    "                    ensemble_model, models, model_types,\n",
    "                    {'X': X_train, 'y': y_train},\n",
    "                    {'X': X_test, 'y': y_test},\n",
    "                    data['feature_names'],\n",
    "                    cv_folds\n",
    "                )\n",
    "                \n",
    "                batch_results[stock_id] = results\n",
    "                \n",
    "                # 保存結果\n",
    "                self._save_results(ensemble_model, results, data['feature_names'])\n",
    "                \n",
    "            # 產生批次處理總結報告\n",
    "            summary = self._generate_batch_summary(batch_results)\n",
    "            self._save_batch_results(batch_results, summary)\n",
    "            \n",
    "            return {\n",
    "                'batch_results': batch_results,\n",
    "                'summary': summary\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"批次處理執行失敗: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_batch_summary(self, batch_results: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"生成批次處理總結報告\"\"\"\n",
    "        try:\n",
    "            summary = {\n",
    "                'total_stocks': len(batch_results),\n",
    "                'success_count': sum(1 for r in batch_results.values() if r is not None),\n",
    "                'average_metrics': {\n",
    "                    'accuracy': [],\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1': [],\n",
    "                    'auc': []\n",
    "                },\n",
    "                'feature_importance_summary': {},\n",
    "                'processing_time': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # 計算平均指標\n",
    "            for results in batch_results.values():\n",
    "                if results:\n",
    "                    for metric in summary['average_metrics'].keys():\n",
    "                        if metric in results:\n",
    "                            summary['average_metrics'][metric].append(results[metric])\n",
    "                    \n",
    "                    # 累加特徵重要性\n",
    "                    if 'feature_importance' in results:\n",
    "                        for feature, importance in results['feature_importance'].items():\n",
    "                            if feature not in summary['feature_importance_summary']:\n",
    "                                summary['feature_importance_summary'][feature] = []\n",
    "                            summary['feature_importance_summary'][feature].append(importance)\n",
    "            \n",
    "            # 計算平均值\n",
    "            for metric in summary['average_metrics'].keys():\n",
    "                values = summary['average_metrics'][metric]\n",
    "                summary['average_metrics'][metric] = (\n",
    "                    float(np.mean(values)) if values else None\n",
    "                )\n",
    "            \n",
    "            # 計算特徵重要性平均值\n",
    "            summary['feature_importance_summary'] = {\n",
    "                feature: float(np.mean(values))\n",
    "                for feature, values in summary['feature_importance_summary'].items()\n",
    "            }\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"生成批次處理總結報告失敗: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_batch_results(self, batch_results: Dict, \n",
    "                          summary: Dict[str, Any]) -> None:\n",
    "        \"\"\"儲存批次處理結果和總結報告\"\"\"\n",
    "        try:\n",
    "            # 儲存總結報告\n",
    "            summary_path = ModelPaths.RESULTS_DIR / 'batch_training_summary.json'\n",
    "            with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(summary, f, indent=4, ensure_ascii=False)\n",
    "                \n",
    "            self.logger.info(f\"批次處理總結報告已儲存至: {summary_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"儲存批次處理結果失敗: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_training_pipeline(self, model_types: List[str] = [\"random_forest\", \"xgboost\"],\n",
    "                            optimize: bool = False, cv_folds: int = 5,\n",
    "                            test_size: float = 0.2) -> Dict[str, Any]:\n",
    "        \"\"\"執行完整的訓練流程\"\"\"\n",
    "        try:\n",
    "            # 數據準備\n",
    "            X, y, feature_names = self._prepare_data()\n",
    "            train_data, test_data = self._split_data(X, y, test_size)\n",
    "            \n",
    "            # 訓練模型\n",
    "            models = []\n",
    "            for model_type in model_types:\n",
    "                model = self._train_model(train_data, model_type, optimize, cv_folds)\n",
    "                models.append(model)\n",
    "            \n",
    "            # 創建並評估集成模型\n",
    "            ensemble_model = EnsembleModel(models)\n",
    "            evaluation_results = self._evaluate_models(\n",
    "                ensemble_model, models, model_types,\n",
    "                train_data, test_data, feature_names, cv_folds\n",
    "            )\n",
    "            \n",
    "            # 保存結果\n",
    "            self._save_results(ensemble_model, evaluation_results, feature_names)\n",
    "            return evaluation_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"訓練流程執行失敗: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _prepare_data(self) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"數據準備和特徵工程\"\"\"\n",
    "        validator = DataValidator(self.stock_id, self.base_path)\n",
    "        stock_data = validator.validate_and_load()\n",
    "        \n",
    "        engineer = FeatureEngineer()\n",
    "        return engineer.process(stock_data)\n",
    "    \n",
    "    def _split_data(self, X: np.ndarray, y: np.ndarray, test_size: float) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"數據分割\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, shuffle=False\n",
    "        )\n",
    "        return {\n",
    "            'X': X_train, \n",
    "            'y': y_train\n",
    "        }, {\n",
    "            'X': X_test, \n",
    "            'y': y_test\n",
    "        }\n",
    "\n",
    "    def _train_model(self, train_data: Dict, model_type: str,\n",
    "                    optimize: bool, cv_folds: int) -> BaseModel:\n",
    "        \"\"\"模型訓練\"\"\"\n",
    "        model = ModelFactory.create_model(model_type)\n",
    "        \n",
    "        if optimize:\n",
    "            optimizer = ModelOptimizer(model_type, self.param_grids[model_type])\n",
    "            best_params = optimizer.optimize(\n",
    "                train_data['X'],\n",
    "                train_data['y'],\n",
    "                cv=cv_folds\n",
    "            )['best_params']\n",
    "            model = ModelFactory.create_model(model_type, params=best_params)\n",
    "            \n",
    "        model.train(train_data['X'], train_data['y'])\n",
    "        return model\n",
    "\n",
    "    def _evaluate_model(self, model: BaseModel, test_data: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"優化模型評估過程\"\"\"\n",
    "        evaluator = ModelEvaluator()\n",
    "        \n",
    "        # 只預測一次\n",
    "        y_prob = model.predict_proba(test_data['X'])\n",
    "        y_pred = (y_prob[:, 1] >= 0.5).astype(int)\n",
    "        \n",
    "        return evaluator.evaluate_classification(\n",
    "            test_data['y'],\n",
    "            y_pred,\n",
    "            y_prob\n",
    "        )\n",
    "    \n",
    "    def _evaluate_models(self, ensemble_model: EnsembleModel, \n",
    "                        individual_models: List[BaseModel],\n",
    "                        model_types: List[str], train_data: Dict,\n",
    "                        test_data: Dict, feature_names: List[str],\n",
    "                        cv_folds: int) -> Dict[str, Any]:\n",
    "        \"\"\"評估所有模型\"\"\"\n",
    "        evaluator = ModelEvaluator()\n",
    "        stability_checker = ModelStabilityChecker()\n",
    "        \n",
    "        results = {\n",
    "            **self._evaluate_model(ensemble_model, test_data),\n",
    "            'model_stability': stability_checker.check_model_stability(\n",
    "                ensemble_model, train_data['X'], train_data['y'], cv_folds\n",
    "            ),\n",
    "            'feature_importance': ensemble_model.get_feature_importance(feature_names),\n",
    "            'individual_model_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # 評估個別模型\n",
    "        for model_type, model in zip(model_types, individual_models):\n",
    "            results['individual_model_metrics'][model_type] = {\n",
    "                'evaluation': self._evaluate_model(model, test_data),\n",
    "                'stability': stability_checker.check_model_stability(\n",
    "                    model, train_data['X'], train_data['y'], cv_folds\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_results(self, model: BaseModel, results: Dict, feature_names: List[str]) -> None:\n",
    "        \"\"\"保存模型和結果\n",
    "        \n",
    "        Args:\n",
    "            model: 要保存的模型\n",
    "            results: 評估結果\n",
    "            feature_names: 特徵名稱列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 獲取保存路徑\n",
    "            model_path = ModelPaths.get_model_path(self.stock_id, model.__class__.__name__)\n",
    "            result_path = ModelPaths.get_result_path(\n",
    "                self.stock_id,\n",
    "                model.__class__.__name__\n",
    "            )\n",
    "\n",
    "            # 更新結果\n",
    "            results.update({\n",
    "                'feature_importance': model.get_feature_importance(feature_names),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "\n",
    "            # 確保目錄存在\n",
    "            model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            result_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # 保存模型\n",
    "            model.save(model_path)\n",
    "            \n",
    "            # 保存結果\n",
    "            with open(result_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            self.logger.info(f\"模型已保存到: {model_path}\")\n",
    "            self.logger.info(f\"結果已保存到: {result_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"保存結果時出錯: {str(e)}\")\n",
    "            # 嘗試清理可能的部分寫入\n",
    "            try:\n",
    "                if Path(model_path).exists():\n",
    "                    Path(model_path).unlink()\n",
    "                if Path(result_path).exists():\n",
    "                    Path(result_path).unlink()\n",
    "            except Exception as cleanup_error:\n",
    "                self.logger.error(f\"清理失敗的保存文件時出錯: {str(cleanup_error)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0135fee8-bd31-436b-a1a3-370559d3ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockScorer:\n",
    "    def calculate_stock_score(self, model_results, historical_data):\n",
    "        \"\"\"計算股票綜合分數\"\"\"\n",
    "        scores = {\n",
    "            'prediction_score': self._calculate_prediction_score(model_results),\n",
    "            'confidence_score': self._calculate_confidence_score(model_results),\n",
    "            'accuracy_score': self._calculate_accuracy_score(model_results),\n",
    "            'risk_score': self._calculate_risk_score(historical_data),\n",
    "            'momentum_score': self._calculate_momentum_score(historical_data)\n",
    "        }\n",
    "        \n",
    "        # 計算加權總分\n",
    "        total_score = sum(score * weight for score, weight in zip(\n",
    "            scores.values(), \n",
    "            [0.3, 0.2, 0.2, 0.15, 0.15]  # 權重可調整\n",
    "        ))\n",
    "        \n",
    "        return total_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c097453a-d715-4768-acf2-ea7889ad324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchModelTrainer:\n",
    "    def train_batch(self, stock_ids: List[str]):\n",
    "        \"\"\"優化的批次訓練\"\"\"\n",
    "        # 分群訓練\n",
    "        stock_groups = self._group_similar_stocks(stock_ids)\n",
    "        \n",
    "        results = {}\n",
    "        for group in stock_groups:\n",
    "            # 對每組使用相似的參數\n",
    "            group_params = self._optimize_group_params(group)\n",
    "            \n",
    "            for stock_id in group:\n",
    "                results[stock_id] = self._train_single_stock(\n",
    "                    stock_id, \n",
    "                    base_params=group_params\n",
    "                )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91f6b0ac-4601-4d85-a6a5-d9fdf84169da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationGenerator:\n",
    "    def generate_recommendations(self, batch_results: Dict):\n",
    "        \"\"\"生成推薦清單\"\"\"\n",
    "        stock_scores = []\n",
    "        \n",
    "        for stock_id, results in batch_results.items():\n",
    "            score, details = self.scorer.calculate_stock_score(\n",
    "                results,\n",
    "                self.historical_data[stock_id]\n",
    "            )\n",
    "            \n",
    "            stock_scores.append({\n",
    "                'stock_id': stock_id,\n",
    "                'total_score': score,\n",
    "                'prediction_probability': results['prediction_proba'],\n",
    "                'confidence': results['model_confidence'],\n",
    "                'risk_level': details['risk_score'],\n",
    "                'score_details': details\n",
    "            })\n",
    "        \n",
    "        # 排序並產生推薦清單\n",
    "        recommendations = sorted(\n",
    "            stock_scores,\n",
    "            key=lambda x: x['total_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6bc6f73-0b3e-4e19-85d0-4b04c3121ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode: str = 'single', stock_ids: Union[str, List[str]] = None, batch_size: int = 1000):\n",
    "    try:\n",
    "        # 設置日誌\n",
    "        logging.basicConfig(\n",
    "            level=logging.WARNING,  # 改為WARNING級別\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('model_training.log', encoding='utf-8'),  # 保存詳細日誌到文件\n",
    "                logging.StreamHandler(sys.stdout)  # 控制台只顯示重要信息\n",
    "            ]\n",
    "        )\n",
    "        # 文件處理器保持INFO級別以記錄詳細信息\n",
    "        file_handler = logging.FileHandler('model_training.log', encoding='utf-8')\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "        \n",
    "        # 初始化路徑\n",
    "        ModelPaths.initialize()\n",
    "        \n",
    "        # 如果是批次模式且未指定股票，則讀取所有股票\n",
    "        if stock_ids is None and mode == 'batch':\n",
    "            feature_file = ModelPaths.META_DIR / \"enhanced_features.csv\"\n",
    "            all_stocks_df = pd.read_csv(feature_file, usecols=['證券代號'])\n",
    "            stock_ids = all_stocks_df['證券代號'].unique().tolist()\n",
    "            logging.info(f\"處理所有股票，共 {len(stock_ids)} 支\")\n",
    "        \n",
    "        # 單一股票模式\n",
    "        if mode == 'single':\n",
    "            stock_id = stock_ids if isinstance(stock_ids, str) else '2330'\n",
    "            trainer = ModelTrainer(stock_id=stock_id)\n",
    "            results = trainer.run_training_pipeline(\n",
    "                model_types=[\"random_forest\", \"xgboost\"],\n",
    "                optimize=True,\n",
    "                cv_folds=5\n",
    "            )\n",
    "            \n",
    "            # 輸出詳細的單一股票分析結果\n",
    "            print_single_stock_results(results)\n",
    "            \n",
    "        # 批次處理模式\n",
    "        elif mode == 'batch':\n",
    "            if isinstance(stock_ids, str):\n",
    "                stock_ids = [stock_ids]\n",
    "                \n",
    "            # 建立批次處理群組\n",
    "            stock_groups = [\n",
    "                stock_ids[i:i + batch_size] \n",
    "                for i in range(0, len(stock_ids), batch_size)\n",
    "            ]\n",
    "            \n",
    "            all_results = {}\n",
    "            for group in stock_groups:\n",
    "                logging.info(f\"處理批次，股票數量: {len(group)}\")\n",
    "                \n",
    "                # 對每個群組進行參數優化和訓練\n",
    "                group_trainer = ModelTrainer(stock_id='')\n",
    "                group_results = group_trainer.run_batch_training(\n",
    "                    stock_ids=group,\n",
    "                    model_types=[\"random_forest\", \"xgboost\"],\n",
    "                    optimize=True,\n",
    "                    cv_folds=5\n",
    "                )\n",
    "                all_results.update(group_results['batch_results'])\n",
    "            \n",
    "            # 儲存訓練結果\n",
    "            save_batch_results(all_results)\n",
    "            \n",
    "            # 輸出批次處理摘要\n",
    "            print_batch_summary(all_results)\n",
    "        \n",
    "        logging.info(\"模型訓練完成\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"執行過程出錯: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def print_single_stock_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"打印單一股票的詳細分析結果\"\"\"\n",
    "    print(\"\\n模型評估結果:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 模型性能指標\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    for metric in metrics:\n",
    "        if metric in results:\n",
    "            print(f\"{metric:15}: {results[metric]:.4f}\")\n",
    "    \n",
    "    # 模型穩定性\n",
    "    if 'model_stability' in results:\n",
    "        print(\"\\n模型穩定性:\")\n",
    "        print(\"-\" * 40)\n",
    "        for metric, value in results['model_stability'].items():\n",
    "            print(f\"{metric:20}: {value:.4f}\")\n",
    "    \n",
    "    # 特徵重要性\n",
    "    if 'feature_importance' in results:\n",
    "        print(\"\\n特徵重要性:\")\n",
    "        print(\"-\" * 40)\n",
    "        sorted_features = sorted(\n",
    "            results['feature_importance'].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        for feature, importance in sorted_features:\n",
    "            print(f\"{feature:20}: {importance:.4f}\")\n",
    "\n",
    "def print_batch_summary(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"打印批次處理的摘要結果\"\"\"\n",
    "    print(\"\\n批次處理摘要:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 計算總體統計\n",
    "    total_stocks = len(results)\n",
    "    successful_stocks = sum(1 for r in results.values() if r is not None)\n",
    "    \n",
    "    print(f\"處理股票總數: {total_stocks}\")\n",
    "    print(f\"成功處理數量: {successful_stocks}\")\n",
    "    \n",
    "    # 計算平均性能指標\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    avg_metrics = {metric: [] for metric in metrics}\n",
    "    \n",
    "    for stock_results in results.values():\n",
    "        if stock_results:\n",
    "            for metric in metrics:\n",
    "                if metric in stock_results:\n",
    "                    avg_metrics[metric].append(stock_results[metric])\n",
    "    \n",
    "    print(\"\\n平均性能指標:\")\n",
    "    for metric, values in avg_metrics.items():\n",
    "        if values:\n",
    "            avg_value = np.mean(values)\n",
    "            print(f\"{metric:15}: {avg_value:.4f}\")\n",
    "\n",
    "def save_batch_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"儲存批次處理結果\"\"\"\n",
    "    save_path = ModelPaths.RESULTS_DIR / f\"batch_training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    # 準備可序列化的結果\n",
    "    serializable_results = {\n",
    "        stock_id: {\n",
    "            k: float(v) if isinstance(v, np.float32) else v\n",
    "            for k, v in result.items()\n",
    "            if k != 'feature_importance'  # 排除特徵重要性，另存為separate file\n",
    "        }\n",
    "        for stock_id, result in results.items()\n",
    "        if result is not None\n",
    "    }\n",
    "    \n",
    "    # 儲存結果\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_results, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    logging.info(f\"批次處理結果已儲存至: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04af509b-67cc-465b-9dec-ef0bf1b47d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 39 個缺失值\n",
      "WARNING:DataValidator:振幅: 3 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 2132 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 39 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 2133 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 1215 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 39 個缺失值\n",
      "WARNING:DataValidator:振幅: 3 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 2138 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 39 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 2139 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 29 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 245 個缺失值\n",
      "WARNING:DataValidator:數據存在缺失值:\n",
      "WARNING:DataValidator:趨勢強度: 19 個缺失值\n",
      "WARNING:DataValidator:振幅: 1 個缺失值\n",
      "WARNING:DataValidator:量能趨勢: 19 個缺失值\n",
      "WARNING:DataValidator:量比: 19 個缺失值\n",
      "WARNING:DataValidator:MACD: 33 個缺失值\n",
      "WARNING:DataValidator:均線糾結度: 29 個缺失值\n",
      "WARNING:DataValidator:波動率: 19 個缺失值\n",
      "WARNING:DataValidator:RSI_動能: 1 個缺失值\n",
      "WARNING:DataValidator:MACD_動能: 34 個缺失值\n",
      "WARNING:DataValidator:本益比_相對值: 308 個缺失值\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "\n",
      "批次處理摘要:\n",
      "----------------------------------------\n",
      "處理股票總數: 10\n",
      "成功處理數量: 10\n",
      "\n",
      "平均性能指標:\n",
      "accuracy       : 0.5153\n",
      "precision      : 0.4993\n",
      "recall         : 0.2581\n",
      "f1             : 0.3207\n",
      "auc            : 0.5100\n"
     ]
    }
   ],
   "source": [
    "main(mode='batch', stock_ids=['2330','3706','2317','2382','3231','2609','2354','6669','3661','3013'], batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da95382c-a091-461b-8561-a86db25689be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 單一股票模式（指定股票）\n",
    "# main(mode='single', stock_ids='2330')\n",
    "\n",
    "# # 2. 批次處理模式（指定股票列表）\n",
    "# main(mode='batch', stock_ids=['2330','3706','2317','2382','3231','2609','2354','6669','3661','3013'], batch_size=2)\n",
    "# # 3. 批次處理模式（處理所有股票）\n",
    "# main(mode='batch')  # 會自動處理所有股票"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
